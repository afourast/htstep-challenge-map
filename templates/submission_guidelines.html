<h2>Test Server Input Guidelines</h2>
<h3>Input</h3>
<p>
  We provide a list of test videos along with their activity label and candidate steps.
  You can find the input file <a href="https://dl.fbaipublicfiles.com/ht-step/test_input.json">here</a>.
  The video IDs correspond to the original YouTube IDs used in HowTo100M.
  For each step, we provide the 'step_headline' and 'step_paragraph' entries, which can be used to ground the step.
</p>
<h3>Example Expected JSON</h3>
<p>
  The test server expects JSON file containing video IDs as keys and lists of single timestamps as values.
  The timestamps in the JSON represent the predictions of your model for grounding the step at the corresponding index in the list.
  The order of the steps aligns with the provided input file, which lists the candidate steps for each test video.
</p>
<pre>
  {
    "---0tKA3iYI": [196, 456, 34, 105, 721, 82],
    "--Gscx86GA0": [0, 295, 509, 143, 612],
    "--KXjWTy7Yw": [200, 48, 831, 630, 402]
  }
</pre>
<p>In this example, we have three video IDs with their corresponding lists of timestamps.</p>
<ul>
  <li>Video ID "---0tKA3iYI" has predictions: [196, 456, 0, 105, 721, 82]</li>
  <li>Video ID "--Gscx86GA0" has predictions: [0, 295, 0, 143, 612]</li>
  <li>Video ID "--KXjWTy7Yw" has predictions: [200, 48, 831, 0, 402]</li>
</ul>
<p>The predicted timestamp for each video will be compared to the ground truth segment annotations.
  Only one prediction should be provided for each candidate step.
  Predictions that fall within any of the ground truth segments for the corresponding step contribute towards higher recall scores.
  Predictions for non-alignable steps will be ignored.
</p>